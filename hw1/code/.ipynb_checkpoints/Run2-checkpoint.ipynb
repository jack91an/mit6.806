{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "from __future__ import division\n",
    "\n",
    "import numpy as np\n",
    "import languagemodel as lm\n",
    "\n",
    "np.random.seed(1)  # for reproducibility\n",
    "\n",
    "corpus_train = lm.readCorpus(\"data/train.txt\")\n",
    "corpus_dev   = lm.readCorpus(\"data/dev.txt\")\n",
    "corpus_test  = lm.readCorpus(\"data/test.txt\")\n",
    "\n",
    "# build a common index (words to integers), mapping rare words (less than 5 occurences) to index 0\n",
    "# nwords = vocabulary size for the models that only see the indexes\n",
    "\n",
    "w2index,nwords = lm.buildIndex(corpus_train+corpus_dev+corpus_test)\n",
    "\n",
    "# find words that appear in the training set so we can deal with new words separately\n",
    "count_train = np.zeros((nwords,))\n",
    "for snt in corpus_train:\n",
    "    for w in snt:\n",
    "        count_train[w2index[w]] += 1\n",
    "\n",
    "# Bigram model as a baseline\n",
    "alpha = 0.1 # add-alpha smoothing\n",
    "probB           = lm.bigramLM(corpus_train, w2index, nwords,alpha)\n",
    "LLB, N          = 0.0, 0\n",
    "bi              = lm.ngramGen(corpus_dev, w2index, 2)\n",
    "for w in bi:\n",
    "    if (count_train[w[1]]>0): # for now, skip target words not seen in training\n",
    "        LLB += np.log(probB[w[0], w[1]])\n",
    "        N += 1\n",
    "print(\"Bi-gram Dev LL = {0}\".format(LLB / N))\n",
    "\n",
    "# Network model\n",
    "print(\"\\nNetwork model training:\")\n",
    "n        = 3    # Length of n-gram \n",
    "dim      = 10   # Word vector dimension\n",
    "hdim     = 30  # Hidden units\n",
    "neurallm = lm.neuralLM(dim, n, hdim, nwords)  # The network model\n",
    "\n",
    "ngrams = lm.ngramGen(corpus_train,w2index,n)\n",
    "ngrams2 = lm.ngramGen(corpus_dev,w2index,n)\n",
    "\n",
    "lrate = 0.5  # Learning rate\n",
    "for it in xrange(10): # passes through the training data\n",
    "    LL, N  = 0.0, 0 # Average log-likelihood, number of ngrams    \n",
    "    for ng in ngrams:\n",
    "        pr = neurallm.update(ng,lrate)\n",
    "        LL += np.log(pr)\n",
    "        N  += 1\n",
    "    print('Train:\\t{0}\\tLL = {1}'.format(it, LL / N)) \n",
    "\n",
    "    #Dev set\n",
    "    LL, N = 0.0, 0 # Average log-likelihood, number of ngrams\n",
    "    for ng in ngrams2:\n",
    "        if (count_train[ng[-1]]>0): # for now, skip target words not seen in training\n",
    "            pr = neurallm.prob(ng)\n",
    "            LL += np.log(pr)\n",
    "            N  += 1\n",
    "    print('Dev:\\t{0}\\tLL = {1}'.format(it, LL / N)) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bi-gram Dev LL = -4.992084628169756\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "from __future__ import division\n",
    "\n",
    "import numpy as np\n",
    "import languagemodel as lm\n",
    "\n",
    "np.random.seed(1)  # for reproducibility\n",
    "\n",
    "corpus_train = lm.readCorpus(\"data/train.txt\")\n",
    "corpus_dev   = lm.readCorpus(\"data/dev.txt\")\n",
    "corpus_test  = lm.readCorpus(\"data/test.txt\")\n",
    "\n",
    "# build a common index (words to integers), mapping rare words (less than 5 occurences) to index 0\n",
    "# nwords = vocabulary size for the models that only see the indexes\n",
    "\n",
    "w2index,nwords = lm.buildIndex(corpus_train+corpus_dev+corpus_test)\n",
    "\n",
    "# find words that appear in the training set so we can deal with new words separately\n",
    "count_train = np.zeros((nwords,))\n",
    "for snt in corpus_train:\n",
    "    for w in snt:\n",
    "        count_train[w2index[w]] += 1\n",
    "\n",
    "# Bigram model as a baseline\n",
    "alpha = 0.1 # add-alpha smoothing\n",
    "probB           = lm.bigramLM(corpus_train, w2index, nwords,alpha)\n",
    "LLB, N          = 0.0, 0\n",
    "bi              = lm.ngramGen(corpus_dev, w2index, 2)\n",
    "for w in bi:\n",
    "    if (count_train[w[1]]>0): # for now, skip target words not seen in training\n",
    "        LLB += np.log(probB[w[0], w[1]])\n",
    "        N += 1\n",
    "print(\"Bi-gram Dev LL = {0}\".format(LLB / N))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Configure baseline bigram model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bi-gram Dev LL = -4.948425066555479 with alpha 0.005\n",
      "Bi-gram Dev LL = -4.883153686434726 with alpha 0.01\n",
      "Bi-gram Dev LL = -4.864619740209547 with alpha 0.015\n",
      "Bi-gram Dev LL = -4.860978607317725 with alpha 0.02\n",
      "Bi-gram Dev LL = -4.863757939080634 with alpha 0.025\n",
      "Bi-gram Dev LL = -4.869712234971148 with alpha 0.030000000000000002\n",
      "Bi-gram Dev LL = -4.877345106194942 with alpha 0.034999999999999996\n",
      "Bi-gram Dev LL = -4.885883828299522 with alpha 0.04\n",
      "Bi-gram Dev LL = -4.89489816262743 with alpha 0.045\n",
      "Bi-gram Dev LL = -4.904135817380125 with alpha 0.049999999999999996\n",
      "Bi-gram Dev LL = -4.913443507489412 with alpha 0.055\n",
      "Bi-gram Dev LL = -4.922725970490986 with alpha 0.06\n",
      "Bi-gram Dev LL = -4.931923344358705 with alpha 0.065\n",
      "Bi-gram Dev LL = -4.940998056193698 with alpha 0.07\n",
      "Bi-gram Dev LL = -4.949926915954853 with alpha 0.07500000000000001\n",
      "Bi-gram Dev LL = -4.958696190725572 with alpha 0.08\n",
      "Bi-gram Dev LL = -4.96729845155322 with alpha 0.085\n",
      "Bi-gram Dev LL = -4.975730508786532 with alpha 0.09000000000000001\n",
      "Bi-gram Dev LL = -4.983992034390634 with alpha 0.095\n",
      "Bi-gram Dev LL = -4.992084628169756 with alpha 0.1\n"
     ]
    }
   ],
   "source": [
    "llh_bi_list = []\n",
    "for alphax in np.linspace(0.005, 0.1, 20):\n",
    "    probB           = lm.bigramLM(corpus_train, w2index, nwords,alphax)\n",
    "    LLB, N          = 0.0, 0\n",
    "    bi              = lm.ngramGen(corpus_dev, w2index, 2)\n",
    "    for w in bi:\n",
    "        if (count_train[w[1]]>0): # for now, skip target words not seen in training\n",
    "            LLB += np.log(probB[w[0], w[1]])\n",
    "            N += 1\n",
    "    print(\"Bi-gram Dev LL = {0} with alpha {1}\".format(LLB / N, alphax))\n",
    "    llh_bi_list.append(LLB / N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.text.Text at 0x10a0e4390>"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYsAAAEZCAYAAABmTgnDAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nWf8P0M0OV4udwiQAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10a6266d8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "plt.plot(np.linspace(0.005, 0.1, 20), llh_bi_list)\n",
    "plt.xlabel(\"alpha\")\n",
    "plt.title(\"average log likelihood of development corpus\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bi-gram Test LL = -4.8457415193597155 with alpha 0.02\n"
     ]
    }
   ],
   "source": [
    "bi_test = lm.ngramGen(corpus_test, w2index, 2)\n",
    "alpha = 0.02\n",
    "probB           = lm.bigramLM(corpus_train, w2index, nwords,alpha)\n",
    "LLB, N          = 0.0, 0\n",
    "\n",
    "for w in bi_test:\n",
    "    if (count_train[w[1]]>0): # for now, skip target words not seen in training\n",
    "        LLB += np.log(probB[w[0], w[1]])\n",
    "        N += 1\n",
    "print(\"Bi-gram Test LL = {0} with alpha {1}\".format(LLB / N, alpha))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n=2, dim=5, hdim=10, dev llh=-4.904785539558707\n",
      "n=2, dim=5, hdim=30, dev llh=-4.843481341786308\n",
      "n=2, dim=5, hdim=50, dev llh=-4.852710258312751\n",
      "n=2, dim=10, hdim=10, dev llh=-4.890904005127846\n",
      "n=2, dim=10, hdim=30, dev llh=-4.794984424580786\n",
      "n=2, dim=10, hdim=50, dev llh=-4.791471388025544\n",
      "n=2, dim=20, hdim=10, dev llh=-4.89460135846894\n",
      "n=2, dim=20, hdim=30, dev llh=-4.78056648744739\n",
      "n=2, dim=20, hdim=50, dev llh=-4.748481825794477\n",
      "n=3, dim=5, hdim=10, dev llh=-4.928681366597829\n",
      "n=3, dim=5, hdim=30, dev llh=-4.7964473688033475\n",
      "n=3, dim=5, hdim=50, dev llh=-4.801392721426568\n",
      "n=3, dim=10, hdim=10, dev llh=-4.856130935224388\n",
      "n=3, dim=10, hdim=30, dev llh=-4.7607189154156435\n",
      "n=3, dim=10, hdim=50, dev llh=-4.7761424349909625\n",
      "n=3, dim=20, hdim=10, dev llh=-4.861990937226091\n",
      "n=3, dim=20, hdim=30, dev llh=-4.755165327400062\n",
      "n=3, dim=20, hdim=50, dev llh=-4.735656115163282\n",
      "n=4, dim=5, hdim=10, dev llh=-4.90576103197\n",
      "n=4, dim=5, hdim=30, dev llh=-4.84448892606861\n",
      "n=4, dim=5, hdim=50, dev llh=-4.817928180287612\n",
      "n=4, dim=10, hdim=10, dev llh=-4.922317775332129\n",
      "n=4, dim=10, hdim=30, dev llh=-4.791019341413742\n",
      "n=4, dim=10, hdim=50, dev llh=-4.781944539915543\n",
      "n=4, dim=20, hdim=10, dev llh=-4.8960483944329445\n",
      "n=4, dim=20, hdim=30, dev llh=-4.798330170885385\n",
      "n=4, dim=20, hdim=50, dev llh=-4.7634669169472215\n",
      "n=5, dim=5, hdim=10, dev llh=-4.991303710743656\n",
      "n=5, dim=5, hdim=30, dev llh=-4.858464873102248\n",
      "n=5, dim=5, hdim=50, dev llh=-4.805399297770563\n",
      "n=5, dim=10, hdim=10, dev llh=-4.8981129917620745\n",
      "n=5, dim=10, hdim=30, dev llh=-4.821929047244881\n",
      "n=5, dim=10, hdim=50, dev llh=-4.77881321472532\n",
      "n=5, dim=20, hdim=10, dev llh=-4.916921096021701\n",
      "n=5, dim=20, hdim=30, dev llh=-4.814204064169429\n",
      "n=5, dim=20, hdim=50, dev llh=-4.794959422042593\n"
     ]
    }
   ],
   "source": [
    "neural_result1 = []\n",
    "\n",
    "iter_num = 2\n",
    "lrate = 0.5  # Learning rate\n",
    "for n in [2,3,4,5]:\n",
    "    ngrams = lm.ngramGen(corpus_train,w2index,n)\n",
    "    ngrams2 = lm.ngramGen(corpus_dev,w2index,n)\n",
    "    for dim in [5, 10, 20]:\n",
    "        for hdim in [10, 30, 50]:\n",
    "            neurallm = lm.neuralLM(dim, n, hdim, nwords)\n",
    "            temp_result = []\n",
    "\n",
    "            for it in range(iter_num): # passes through the training data\n",
    "                for ng in ngrams:\n",
    "                    pr = neurallm.update(ng,lrate)\n",
    "\n",
    "                #Dev set\n",
    "                LL, N = 0.0, 0 # Average log-likelihood, number of ngrams\n",
    "                for ng in ngrams2:\n",
    "                    if (count_train[ng[-1]]>0): # for now, skip target words not seen in training\n",
    "                        pr = neurallm.prob(ng)\n",
    "                        LL += np.log(pr)\n",
    "                        N  += 1\n",
    "                temp_result.append(LL/N)\n",
    "            print('n={0}, dim={1}, hdim={2}, dev llh={3}'.format(n, dim, hdim, np.mean(temp_result)))\n",
    "            neural_result1.append([n, dim, hdim, temp_result])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n=3, dim=50, hdim=40, dev llh=-4.615536150004457\n",
      "n=3, dim=50, hdim=50, dev llh=-4.6432565387426985\n",
      "n=3, dim=50, hdim=60, dev llh=-4.630532437885575\n",
      "n=3, dim=75, hdim=40, dev llh=-4.6397915786224555\n",
      "n=3, dim=75, hdim=50, dev llh=-4.626198648142621\n",
      "n=3, dim=75, hdim=60, dev llh=-4.607094358766884\n",
      "n=3, dim=100, hdim=40, dev llh=-4.623066747344818\n",
      "n=3, dim=100, hdim=50, dev llh=-4.63456676862269\n",
      "n=3, dim=100, hdim=60, dev llh=-4.626712758690609\n"
     ]
    }
   ],
   "source": [
    "neural_result2 = []\n",
    "\n",
    "iter_num = 10\n",
    "lrate = 0.5  # Learning rate\n",
    "for n in [3]:\n",
    "    ngrams = lm.ngramGen(corpus_train,w2index,n)\n",
    "    ngrams2 = lm.ngramGen(corpus_dev,w2index,n)\n",
    "    for dim in [50, 75, 100]:\n",
    "        for hdim in [40, 50, 60]:\n",
    "            neurallm = lm.neuralLM(dim, n, hdim, nwords)\n",
    "            temp_result = []\n",
    "\n",
    "            for it in range(iter_num): # passes through the training data\n",
    "                for ng in ngrams:\n",
    "                    pr = neurallm.update(ng,lrate)\n",
    "\n",
    "                #Dev set\n",
    "                LL, N = 0.0, 0 # Average log-likelihood, number of ngrams\n",
    "                for ng in ngrams2:\n",
    "                    if (count_train[ng[-1]]>0): # for now, skip target words not seen in training\n",
    "                        pr = neurallm.prob(ng)\n",
    "                        LL += np.log(pr)\n",
    "                        N  += 1\n",
    "                temp_result.append(LL/N)\n",
    "            print('n={0}, dim={1}, hdim={2}, dev llh={3}'.format(n, dim, hdim, np.mean(temp_result)))\n",
    "            neural_result2.append([n, dim, hdim, temp_result])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n=3, dim=75, hdim=60, dev llh=-4.6397105792343725\n",
      "n=3, dim=75, hdim=70, dev llh=-4.632634290665759\n",
      "n=3, dim=75, hdim=80, dev llh=-4.6349324644524925\n"
     ]
    }
   ],
   "source": [
    "neural_result2 = []\n",
    "\n",
    "iter_num = 5\n",
    "lrate = 0.5  # Learning rate\n",
    "for n in [3]:\n",
    "    ngrams = lm.ngramGen(corpus_train,w2index,n)\n",
    "    ngrams2 = lm.ngramGen(corpus_dev,w2index,n)\n",
    "    for dim in [75]:\n",
    "        for hdim in [60,70,80]:\n",
    "            neurallm = lm.neuralLM(dim, n, hdim, nwords)\n",
    "            temp_result = []\n",
    "\n",
    "            for it in range(iter_num): # passes through the training data\n",
    "                for ng in ngrams:\n",
    "                    pr = neurallm.update(ng,lrate)\n",
    "\n",
    "                #Dev set\n",
    "                LL, N = 0.0, 0 # Average log-likelihood, number of ngrams\n",
    "                for ng in ngrams2:\n",
    "                    if (count_train[ng[-1]]>0): # for now, skip target words not seen in training\n",
    "                        pr = neurallm.prob(ng)\n",
    "                        LL += np.log(pr)\n",
    "                        N  += 1\n",
    "                temp_result.append(LL/N)\n",
    "            print('n={0}, dim={1}, hdim={2}, dev llh={3}'.format(n, dim, hdim, np.mean(temp_result)))\n",
    "            neural_result2.append([n, dim, hdim, temp_result])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n=3, dim=75, hdim=70, dev llh=-4.6563715623237485\n"
     ]
    }
   ],
   "source": [
    "neural_result3 = []\n",
    "\n",
    "iter_num = 10\n",
    "lrate = 0.5  # Learning rate\n",
    "for n in [3]:\n",
    "    ngrams = lm.ngramGen(corpus_train,w2index,n)\n",
    "    ngrams2 = lm.ngramGen(corpus_dev,w2index,n)\n",
    "    ngrams_test = lm.ngramGen(corpus_test,w2index,n)\n",
    "    for dim in [75]:\n",
    "        for hdim in [70]:\n",
    "            neurallm = lm.neuralLM(dim, n, hdim, nwords)\n",
    "            temp_result = []\n",
    "\n",
    "            for it in range(iter_num): # passes through the training data\n",
    "                for ng in ngrams:\n",
    "                    pr = neurallm.update(ng,lrate)\n",
    "\n",
    "                #Dev set\n",
    "                LL, N = 0.0, 0 # Average log-likelihood, number of ngrams\n",
    "                for ng in ngrams_test:\n",
    "                    if (count_train[ng[-1]]>0): # for now, skip target words not seen in training\n",
    "                        pr = neurallm.prob(ng)\n",
    "                        LL += np.log(pr)\n",
    "                        N  += 1\n",
    "                temp_result.append(LL/N)\n",
    "            print('n={0}, dim={1}, hdim={2}, dev llh={3}'.format(n, dim, hdim, np.mean(temp_result)))\n",
    "            neural_result3.append([n, dim, hdim, temp_result])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "neural_result4 = []\n",
    "\n",
    "iter_num = 5\n",
    "lrate = 0.5  # Learning rate\n",
    "for n in [3]:\n",
    "    ngrams = lm.ngramGen(corpus_train,w2index,n)\n",
    "    ngrams2 = lm.ngramGen(corpus_dev,w2index,n)\n",
    "    ngrams_test = lm.ngramGen(corpus_test,w2index,n)\n",
    "    for dim in [10]:\n",
    "        for hdim in [10, 30, 50, 75, 100, 200]:\n",
    "            neurallm = lm.neuralLM(dim, n, hdim, nwords)\n",
    "            temp_result = []\n",
    "\n",
    "            for it in range(iter_num): # passes through the training data\n",
    "                for ng in ngrams:\n",
    "                    pr = neurallm.update(ng,lrate)\n",
    "\n",
    "                #Dev set\n",
    "                LL, N = 0.0, 0 # Average log-likelihood, number of ngrams\n",
    "                for ng in ngrams_test:\n",
    "                    if (count_train[ng[-1]]>0): # for now, skip target words not seen in training\n",
    "                        pr = neurallm.prob(ng)\n",
    "                        LL += np.log(pr)\n",
    "                        N  += 1\n",
    "                temp_result.append(LL/N)\n",
    "            print('n={0}, dim={1}, hdim={2}, dev llh={3}'.format(n, dim, hdim, np.mean(temp_result)))\n",
    "            neural_result4.append([n, dim, hdim, np.mean(temp_result), temp_result])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_stn_1 = [[\"the\", \"choice\", \"of\", \"their\", \"class\", \"is\", \"good\"]]\n",
    "test_stn_2 = [[\"the\", \"choice\", \"of\", \"there\", \"class\", \"is\", \"good\"]]\n",
    "\n",
    "\n",
    "test_trigram_1 = lm.ngramGen(test_stn_1,w2index,3)\n",
    "test_trigram_2 = lm.ngramGen(test_stn_2,w2index,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1418, 1417, 620],\n",
       " [1417, 620, 1092],\n",
       " [620, 1092, 912],\n",
       " [1092, 912, 1056],\n",
       " [912, 1056, 0],\n",
       " [1056, 0, 230],\n",
       " [0, 230, 598]]"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_trigram_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-5.26790220051\n"
     ]
    }
   ],
   "source": [
    "LL, N = 0.0, 0 # Average log-likelihood, number of ngrams\n",
    "for ng in test_trigram_1:\n",
    "    pr = neurallm.prob(ng)\n",
    "    LL += np.log(pr)\n",
    "    N  += 1\n",
    "print(LL/N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-6.05290330802\n"
     ]
    }
   ],
   "source": [
    "LL, N = 0.0, 0 # Average log-likelihood, number of ngrams\n",
    "for ng in test_trigram_2:\n",
    "    pr = neurallm.prob(ng)\n",
    "    LL += np.log(pr)\n",
    "    N  += 1\n",
    "print(LL/N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Network model training:\n",
      "Train:\t0\tLL = -5.183838706612703\n",
      "Dev:\t0\tLL = -4.83666408983967\n",
      "Train:\t1\tLL = -4.630877101085274\n",
      "Dev:\t1\tLL = -4.7168726314392515\n",
      "Train:\t2\tLL = -4.466241028270229\n",
      "Dev:\t2\tLL = -4.668948482547839\n",
      "Train:\t3\tLL = -4.367345741095842\n",
      "Dev:\t3\tLL = -4.644548897022256\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-55-1d06c403e836>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mLL\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mN\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;31m# Average log-likelihood, number of ngrams\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mng\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mngrams\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0mpr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mneurallm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mng\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlrate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m         \u001b[0mLL\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mN\u001b[0m  \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/fqian/mit6.806/hw1/code/languagemodel.py\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, ngram, lrate)\u001b[0m\n\u001b[1;32m    160\u001b[0m         \u001b[0mpr\u001b[0m       \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputL\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m         \u001b[0;31m# Backpropagate (and update layers)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 162\u001b[0;31m         \u001b[0mdh\u001b[0m       \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputL\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackprop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlrate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    163\u001b[0m         \u001b[0mdx\u001b[0m       \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhiddenL\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackprop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlrate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m         \u001b[0;31m# Update word vectors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/fqian/mit6.806/hw1/code/languagemodel.py\u001b[0m in \u001b[0;36mbackprop\u001b[0;34m(self, x, lrate, y)\u001b[0m\n\u001b[1;32m    104\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mG2\u001b[0m   \u001b[0;34m+=\u001b[0m \u001b[0mxnorm2\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mgrad\u001b[0m \u001b[0;34m**\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mWo\u001b[0m   \u001b[0;34m+=\u001b[0m \u001b[0mlrate\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mgrad\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mG2o\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mW\u001b[0m    \u001b[0;34m+=\u001b[0m \u001b[0mlrate\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mouter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mG2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    107\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mxdelta\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda/lib/python3.5/site-packages/numpy/core/numeric.py\u001b[0m in \u001b[0;36mouter\u001b[0;34m(a, b, out)\u001b[0m\n\u001b[1;32m   1091\u001b[0m     \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1092\u001b[0m     \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1093\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mmultiply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnewaxis\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnewaxis\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1094\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1095\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Network model\n",
    "print(\"\\nNetwork model training:\")\n",
    "n        = 3    # Length of n-gram \n",
    "dim      = 50   # Word vector dimension\n",
    "hdim     = 100  # Hidden units\n",
    "neurallm = lm.neuralLM(dim, n, hdim, nwords)  # The network model\n",
    "\n",
    "ngrams = lm.ngramGen(corpus_train,w2index,n)\n",
    "ngrams2 = lm.ngramGen(corpus_dev,w2index,n)\n",
    "\n",
    "lrate = 0.5  # Learning rate\n",
    "for it in range(5): # passes through the training data\n",
    "    LL, N  = 0.0, 0 # Average log-likelihood, number of ngrams    \n",
    "    for ng in ngrams:\n",
    "        pr = neurallm.update(ng,lrate)\n",
    "        LL += np.log(pr)\n",
    "        N  += 1\n",
    "    print('Train:\\t{0}\\tLL = {1}'.format(it, LL / N)) \n",
    "\n",
    "    #Dev set\n",
    "    LL, N = 0.0, 0 # Average log-likelihood, number of ngrams\n",
    "    for ng in ngrams2:\n",
    "        if (count_train[ng[-1]]>0): # for now, skip target words not seen in training\n",
    "            pr = neurallm.prob(ng)\n",
    "            LL += np.log(pr)\n",
    "            N  += 1\n",
    "    print('Dev:\\t{0}\\tLL = {1}'.format(it, LL / N)) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "python2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
