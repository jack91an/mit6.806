{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "from __future__ import division\n",
    "\n",
    "import numpy as np\n",
    "import languagemodel as lm\n",
    "\n",
    "np.random.seed(1)  # for reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "corpus_train = lm.readCorpus(\"data/train.txt\")\n",
    "corpus_dev   = lm.readCorpus(\"data/dev.txt\")\n",
    "corpus_test  = lm.readCorpus(\"data/test.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# build a common index (words to integers), mapping rare words (less than 5 occurences) to index 0\n",
    "# nwords = vocabulary size for the models that only see the indexes\n",
    "\n",
    "w2index,nwords = lm.buildIndex(corpus_train+corpus_dev+corpus_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'frank': 1,\n",
       " 'extension': 0,\n",
       " 'anti-takeover': 0,\n",
       " 'plain': 0,\n",
       " 'recover': 0,\n",
       " 'air': 2,\n",
       " 'watchers': 0,\n",
       " 'result': 3,\n",
       " 'vivid': 0,\n",
       " 'failure': 0,\n",
       " 'raw': 0,\n",
       " 'belief': 4,\n",
       " 'repeat': 0,\n",
       " 'plants': 5,\n",
       " 'quarter': 6,\n",
       " 'represented': 0,\n",
       " 'nato': 0,\n",
       " 'unlike': 7,\n",
       " 'emerges': 0,\n",
       " 'member': 8,\n",
       " 'achieved': 0,\n",
       " 'reagan': 0,\n",
       " 'booked': 0,\n",
       " 'switzerland': 0,\n",
       " 'stick': 0,\n",
       " 'postpone': 0,\n",
       " 'acceptance': 0,\n",
       " 'desire': 0,\n",
       " 'texas': 9,\n",
       " 'california': 10,\n",
       " 'fire': 0,\n",
       " 'dynamics': 0,\n",
       " 'paying': 0,\n",
       " 'minorities': 0,\n",
       " 'using': 0,\n",
       " 'jurisdiction': 0,\n",
       " 'apple': 13,\n",
       " 'discounting': 0,\n",
       " 'caused': 14,\n",
       " 'watching': 0,\n",
       " 'rumors': 0,\n",
       " 'bidding': 0,\n",
       " 'diet': 0,\n",
       " 'redemption': 0,\n",
       " 'exchange': 15,\n",
       " 'attitude': 0,\n",
       " 'highly': 0,\n",
       " 'turned': 16,\n",
       " 'alongside': 0,\n",
       " 'scheme': 0,\n",
       " 'frequently': 0,\n",
       " 'bargain': 0,\n",
       " 'sort': 0,\n",
       " 'reacted': 0,\n",
       " 'differ': 0,\n",
       " 'candidates': 0,\n",
       " 'remained': 0,\n",
       " 'preceding': 0,\n",
       " 'book': 0,\n",
       " 'week': 17,\n",
       " 'consensus': 0,\n",
       " 'dog': 0,\n",
       " 'accept': 0,\n",
       " 'gramm-rudman': 0,\n",
       " 'friend': 0,\n",
       " 'hire': 0,\n",
       " 'interpreted': 0,\n",
       " 'peaked': 0,\n",
       " 'fear': 0,\n",
       " 'meridian': 0,\n",
       " 'releases': 0,\n",
       " 'headquarters': 0,\n",
       " 'europe': 18,\n",
       " 'disclosed': 19,\n",
       " 'schools': 0,\n",
       " 'erode': 0,\n",
       " 'count': 0,\n",
       " 'hartford': 0,\n",
       " 'preliminary': 0,\n",
       " 'mosbacher': 0,\n",
       " 'name': 20,\n",
       " 'suburban': 0,\n",
       " 'weekend': 141,\n",
       " 'basically': 0,\n",
       " 'doubt': 149,\n",
       " 'painewebber': 0,\n",
       " 'net': 22,\n",
       " 'gave': 0,\n",
       " 'co': 23,\n",
       " 'averaged': 0,\n",
       " 'team': 0,\n",
       " 'resume': 0,\n",
       " 'contends': 0,\n",
       " 'supplying': 0,\n",
       " 'speed': 0,\n",
       " 'efforts': 24,\n",
       " 'rebates': 0,\n",
       " 'standard': 25,\n",
       " 'quotations': 0,\n",
       " 'changed': 0,\n",
       " 'roberts': 0,\n",
       " 'rain': 0,\n",
       " 'damage': 27,\n",
       " 'returns': 0,\n",
       " 'alaska': 0,\n",
       " 'tandy': 0,\n",
       " 'slow': 0,\n",
       " 'simpson': 28,\n",
       " 'gene': 0,\n",
       " 'contained': 0,\n",
       " 'resistance': 0,\n",
       " 'formal': 0,\n",
       " 'publisher': 0,\n",
       " 'switched': 0,\n",
       " 'ready': 29,\n",
       " 'replied': 0,\n",
       " 'historical': 0,\n",
       " 'half-hour': 0,\n",
       " 'carrying': 0,\n",
       " 'wears': 0,\n",
       " 'manager': 30,\n",
       " 'functions': 0,\n",
       " 'plane': 0,\n",
       " 'keep': 31,\n",
       " 'mercantile': 0,\n",
       " 'earnings': 32,\n",
       " 'agreeing': 0,\n",
       " 'bebear': 0,\n",
       " 'coping': 0,\n",
       " 'rights': 33,\n",
       " 'character': 0,\n",
       " 'half': 34,\n",
       " 'including': 35,\n",
       " 'backer': 36,\n",
       " 'confusing': 0,\n",
       " 'minimum': 0,\n",
       " 'marginal': 0,\n",
       " 'nielsen': 0,\n",
       " 'tentative': 0,\n",
       " 'two-thirds': 0,\n",
       " 'league': 0,\n",
       " 'kong': 439,\n",
       " 'mass': 0,\n",
       " 'sellers': 0,\n",
       " 'aides': 0,\n",
       " 'providing': 0,\n",
       " 'mainframes': 0,\n",
       " 'produce': 38,\n",
       " 'over': 39,\n",
       " 'non-u.s.': 0,\n",
       " 'thursday': 40,\n",
       " 'year-end': 0,\n",
       " 'slowed': 0,\n",
       " 'say': 41,\n",
       " 'program-trading': 0,\n",
       " 'jay': 0,\n",
       " 'frozen': 0,\n",
       " 'telegraph': 0,\n",
       " 'headed': 42,\n",
       " 'conglomerate': 0,\n",
       " 'dated': 0,\n",
       " 'injured': 0,\n",
       " 'remainder': 0,\n",
       " 'discrimination': 0,\n",
       " 'offsetting': 0,\n",
       " 'waste': 0,\n",
       " 'connected': 0,\n",
       " 'plastic': 0,\n",
       " 'consider': 43,\n",
       " 'subway': 0,\n",
       " 'worry': 0,\n",
       " 'encourage': 0,\n",
       " 'town': 44,\n",
       " 'tied': 0,\n",
       " 'sheep': 0,\n",
       " 'cadillac': 0,\n",
       " 'further': 45,\n",
       " 'missouri': 0,\n",
       " 'resulting': 0,\n",
       " 'fully': 0,\n",
       " 'brewing': 0,\n",
       " 'learn': 0,\n",
       " 'attempted': 0,\n",
       " 'strengthen': 0,\n",
       " 'again': 46,\n",
       " 'offshore': 0,\n",
       " 'alternatives': 0,\n",
       " 'sister': 0,\n",
       " 'leadership': 0,\n",
       " 'quota': 48,\n",
       " 'wins': 0,\n",
       " 'continue': 50,\n",
       " 'pays': 0,\n",
       " 'promote': 0,\n",
       " 'elected': 0,\n",
       " 'officials': 51,\n",
       " 'soviet': 52,\n",
       " 'soft': 0,\n",
       " 'nec': 0,\n",
       " 'hedge': 0,\n",
       " 'cooperation': 0,\n",
       " 'predicted': 53,\n",
       " 'nights': 0,\n",
       " 'korean': 0,\n",
       " 'maine': 0,\n",
       " 'channels': 0,\n",
       " 'inability': 0,\n",
       " 'reserve': 718,\n",
       " 'left': 230,\n",
       " 'usual': 0,\n",
       " 'committee': 55,\n",
       " 'approvals': 0,\n",
       " 'breaker': 0,\n",
       " 'pegged': 0,\n",
       " 'worried': 56,\n",
       " 'valuable': 0,\n",
       " 'pachinko': 0,\n",
       " 'prompt': 0,\n",
       " 'monday': 58,\n",
       " 'insolvent': 0,\n",
       " 'danger': 0,\n",
       " 'picked': 0,\n",
       " 'governor': 0,\n",
       " 'rumor': 0,\n",
       " 'wash.': 0,\n",
       " 'businessmen': 0,\n",
       " 'restated': 0,\n",
       " 'revision': 0,\n",
       " 'maker': 59,\n",
       " 'disproportionate': 0,\n",
       " 'violations': 0,\n",
       " 'disappear': 0,\n",
       " 'besieged': 0,\n",
       " 'format': 0,\n",
       " 'dating': 0,\n",
       " 'lackluster': 0,\n",
       " \"'ll\": 61,\n",
       " 'totally': 0,\n",
       " 'bradford': 0,\n",
       " 'outside': 62,\n",
       " 'package': 0,\n",
       " 'help': 63,\n",
       " 'boesky': 64,\n",
       " 'rated': 0,\n",
       " 'companies': 65,\n",
       " 'benefits': 66,\n",
       " 'cite': 0,\n",
       " 'promised': 0,\n",
       " 'lead': 68,\n",
       " 'generate': 0,\n",
       " 'ask': 0,\n",
       " 'friendly': 0,\n",
       " 'abuse': 0,\n",
       " 'environment': 0,\n",
       " 'texaco': 0,\n",
       " 'crime': 0,\n",
       " 'broadcasting': 69,\n",
       " 'bargains': 0,\n",
       " 'abandon': 0,\n",
       " 'prospect': 0,\n",
       " 'commentary': 0,\n",
       " 'occurred': 0,\n",
       " 'intense': 0,\n",
       " 'printed': 0,\n",
       " 'grim': 0,\n",
       " 'okla.': 0,\n",
       " 'affect': 0,\n",
       " 'hear': 0,\n",
       " 'netherlands': 0,\n",
       " 'urge': 0,\n",
       " 'va.': 0,\n",
       " 'disruptions': 0,\n",
       " 'excluding': 0,\n",
       " 'force': 70,\n",
       " 'society': 0,\n",
       " 'process': 0,\n",
       " 'foods': 0,\n",
       " 'l.': 0,\n",
       " 'lesser': 0,\n",
       " 'flags': 0,\n",
       " 'bond': 488,\n",
       " 'claimed': 0,\n",
       " 'coin': 0,\n",
       " 'section': 0,\n",
       " 'goal': 0,\n",
       " 'integrated': 0,\n",
       " 'subsidized': 0,\n",
       " 'fighting': 0,\n",
       " 'nov.': 0,\n",
       " 'rally': 236,\n",
       " 'brains': 0,\n",
       " 'baby': 0,\n",
       " 'quick': 72,\n",
       " 'delay': 73,\n",
       " 'stations': 0,\n",
       " 'suffered': 0,\n",
       " 'beverly': 0,\n",
       " 'gain': 74,\n",
       " 'management': 75,\n",
       " 'company': 76,\n",
       " 'politics': 0,\n",
       " 'contract': 77,\n",
       " 'do': 78,\n",
       " 'darman': 79,\n",
       " 'side': 80,\n",
       " 'disadvantage': 0,\n",
       " 'investigator': 0,\n",
       " 'unusual': 0,\n",
       " 'reopened': 0,\n",
       " 'arbitrage': 0,\n",
       " 'comfortable': 0,\n",
       " 'alan': 0,\n",
       " 'put': 81,\n",
       " 'voice': 0,\n",
       " 'plot': 0,\n",
       " 'cement': 0,\n",
       " 'exercise': 0,\n",
       " 'sears': 0,\n",
       " 'positions': 82,\n",
       " 'discussing': 0,\n",
       " 'house': 83,\n",
       " 'offers': 84,\n",
       " 'provided': 0,\n",
       " 'illegally': 0,\n",
       " 'relations': 0,\n",
       " 'even': 86,\n",
       " 'territory': 0,\n",
       " 'owner': 0,\n",
       " 'magnified': 0,\n",
       " 'looking': 87,\n",
       " 'hung': 0,\n",
       " 'period': 88,\n",
       " 'professor': 0,\n",
       " 'publishers': 0,\n",
       " 'crucial': 0,\n",
       " 'soybean': 0,\n",
       " 'conclude': 0,\n",
       " 'committed': 0,\n",
       " 'gross': 0,\n",
       " 'assembled': 0,\n",
       " 'defense': 891,\n",
       " 'detergent': 89,\n",
       " 'supreme': 0,\n",
       " 'managing': 0,\n",
       " 'beginning': 90,\n",
       " 'treatment': 0,\n",
       " 'high-technology': 0,\n",
       " 'ranks': 0,\n",
       " 'quit': 0,\n",
       " 'unpaid': 0,\n",
       " 'deputy': 0,\n",
       " 'approve': 0,\n",
       " 'playing': 0,\n",
       " '1970s': 0,\n",
       " 'leslie': 0,\n",
       " 'crackdown': 0,\n",
       " 'wages': 0,\n",
       " 'desperate': 0,\n",
       " 'permits': 0,\n",
       " 'investing': 0,\n",
       " 'determine': 0,\n",
       " 'these': 92,\n",
       " 'goes': 0,\n",
       " 'measures': 93,\n",
       " 'burden': 0,\n",
       " 'pointed': 0,\n",
       " 'overdue': 0,\n",
       " 'southern': 0,\n",
       " 'retreat': 0,\n",
       " 'urging': 0,\n",
       " 'knowledgeable': 0,\n",
       " 'swing': 0,\n",
       " 'department-store': 0,\n",
       " 'boeing': 94,\n",
       " 'thrifts': 95,\n",
       " 'man': 96,\n",
       " 'taxable': 0,\n",
       " 'cause': 97,\n",
       " 'trust': 98,\n",
       " 'dealership': 0,\n",
       " 'trillion': 0,\n",
       " 'p.m.': 99,\n",
       " 'procedural': 0,\n",
       " 'domestic': 100,\n",
       " 'justices': 101,\n",
       " 'only': 102,\n",
       " 'venture': 103,\n",
       " 'ensuring': 0,\n",
       " 'minds': 0,\n",
       " 'respond': 0,\n",
       " 'main': 104,\n",
       " 'government': 105,\n",
       " 'bloomingdale': 0,\n",
       " 'importance': 0,\n",
       " 'plc': 107,\n",
       " 'sector': 0,\n",
       " 'deal': 108,\n",
       " 'car': 109,\n",
       " 'except': 0,\n",
       " 'doctrine': 0,\n",
       " 'pharmaceutical': 110,\n",
       " 'removing': 0,\n",
       " 'negative': 0,\n",
       " 'leonard': 0,\n",
       " 'utilization': 0,\n",
       " 'sorts': 0,\n",
       " 'bound': 0,\n",
       " 'doing': 111,\n",
       " 'seasonal': 0,\n",
       " 'tumultuous': 0,\n",
       " 'entertainment': 0,\n",
       " 'news': 113,\n",
       " 'breakdown': 0,\n",
       " 'divisions': 0,\n",
       " 'colleagues': 0,\n",
       " 'improvement': 0,\n",
       " 'heavily': 0,\n",
       " 'provider': 0,\n",
       " 'fact': 114,\n",
       " 'benefit': 115,\n",
       " 'quite': 0,\n",
       " 'looks': 0,\n",
       " 'judge': 0,\n",
       " 'conceded': 0,\n",
       " 'signal': 245,\n",
       " 'look': 116,\n",
       " 'adviser': 0,\n",
       " 'ibm': 117,\n",
       " 'rout': 0,\n",
       " 'resist': 0,\n",
       " 'hang': 0,\n",
       " 'undoubtedly': 0,\n",
       " 'soared': 0,\n",
       " 'pounds': 0,\n",
       " 'medicine': 0,\n",
       " 'august': 118,\n",
       " 'ton': 0,\n",
       " 'lifetime': 0,\n",
       " 'paris': 0,\n",
       " 'distributor': 0,\n",
       " 'supposed': 0,\n",
       " 'clearance': 0,\n",
       " 'displays': 0,\n",
       " 'reliance': 0,\n",
       " 'cup': 0,\n",
       " 'employer': 0,\n",
       " 'currently': 728,\n",
       " 'leon': 0,\n",
       " 'hands': 0,\n",
       " 'indianapolis': 0,\n",
       " 'treasurys': 0,\n",
       " 'pa.': 0,\n",
       " 'totaled': 119,\n",
       " 'galileo': 0,\n",
       " 'bruce': 120,\n",
       " 'strips': 0,\n",
       " 'styles': 0,\n",
       " 'cold': 0,\n",
       " 'beaten': 0,\n",
       " 'release': 0,\n",
       " 'productive': 0,\n",
       " 'recommended': 0,\n",
       " 'confirm': 0,\n",
       " 'subcommittee': 0,\n",
       " 'step': 0,\n",
       " 'supplies': 121,\n",
       " 'where': 122,\n",
       " 'sudden': 0,\n",
       " 'gained': 0,\n",
       " 'professionals': 0,\n",
       " 'george': 124,\n",
       " 'mci': 0,\n",
       " 'they': 125,\n",
       " 'replacing': 0,\n",
       " 'jerry': 0,\n",
       " 'going': 729,\n",
       " 'electronic': 0,\n",
       " 'government-owned': 0,\n",
       " 'accompanied': 0,\n",
       " 'fires': 0,\n",
       " 'pit': 0,\n",
       " 'mistake': 0,\n",
       " 'hide': 0,\n",
       " 'scotland': 0,\n",
       " 'pursue': 0,\n",
       " 'generale': 0,\n",
       " 'whether': 128,\n",
       " 'mccaw': 0,\n",
       " 'chains': 0,\n",
       " 'friday': 129,\n",
       " 'tide': 0,\n",
       " 'illegal': 0,\n",
       " 'so': 130,\n",
       " 'repairs': 0,\n",
       " 'catholic': 0,\n",
       " 'otc': 131,\n",
       " 'giants': 0,\n",
       " 'number': 132,\n",
       " 'massacre': 0,\n",
       " 'since': 133,\n",
       " 'action': 134,\n",
       " 'reported': 135,\n",
       " 'meetings': 0,\n",
       " 'fast-food': 0,\n",
       " 'robinson': 0,\n",
       " 'falls': 0,\n",
       " 'build': 136,\n",
       " 'adapted': 0,\n",
       " 'talked': 0,\n",
       " 'microsoft': 0,\n",
       " 'skeptical': 0,\n",
       " 'while': 138,\n",
       " 'arnold': 0,\n",
       " 'observed': 0,\n",
       " 'partners': 0,\n",
       " 'widespread': 0,\n",
       " 'giuliani': 0,\n",
       " 'identity': 0,\n",
       " 'range': 139,\n",
       " 'facility': 0,\n",
       " 'holding': 140,\n",
       " 'sit': 0,\n",
       " 'asserted': 0,\n",
       " 'accepting': 0,\n",
       " 'eroded': 0,\n",
       " 'operating': 142,\n",
       " 'italy': 143,\n",
       " 'note': 0,\n",
       " 'million': 144,\n",
       " 'consent': 0,\n",
       " 'change': 145,\n",
       " 'capital': 146,\n",
       " 'heat': 0,\n",
       " 'dropping': 0,\n",
       " 'dozens': 0,\n",
       " 'labor': 147,\n",
       " 'grants': 0,\n",
       " 'probe': 0,\n",
       " 'influence': 0,\n",
       " 'restructuring': 0,\n",
       " 'least': 148,\n",
       " 'proposing': 0,\n",
       " 'shed': 0,\n",
       " 'several': 21,\n",
       " 'books': 0,\n",
       " 'reaction': 150,\n",
       " 'suburb': 0,\n",
       " 'technique': 0,\n",
       " 'links': 0,\n",
       " 'theories': 0,\n",
       " 'hailed': 0,\n",
       " 'phones': 0,\n",
       " 'digest': 0,\n",
       " 'depositary': 0,\n",
       " 'softer': 0,\n",
       " 'beleaguered': 0,\n",
       " 'realize': 0,\n",
       " 'lincoln': 0,\n",
       " 'vaccine': 0,\n",
       " 'expenditures': 0,\n",
       " 'bottom': 151,\n",
       " 'version': 0,\n",
       " 'convention': 0,\n",
       " 'moving': 0,\n",
       " 'p.': 0,\n",
       " 'instituted': 0,\n",
       " 'advanced': 0,\n",
       " 'newsletter': 0,\n",
       " 'prudential': 0,\n",
       " 'comfort': 0,\n",
       " 'shoot': 0,\n",
       " 'printing': 0,\n",
       " 'speculative': 0,\n",
       " 'secondary': 0,\n",
       " 'capita': 0,\n",
       " 'adequate': 0,\n",
       " 'intensify': 0,\n",
       " 'woman': 0,\n",
       " 'sharpest': 0,\n",
       " 'yale': 0,\n",
       " 'globe': 0,\n",
       " 'her': 152,\n",
       " 'identical': 0,\n",
       " 'indeed': 153,\n",
       " 'excess': 0,\n",
       " 'projects': 154,\n",
       " 'tremendous': 0,\n",
       " 'plays': 0,\n",
       " 'carefully': 0,\n",
       " 'bancorp': 0,\n",
       " 'midland': 0,\n",
       " 'reacting': 0,\n",
       " 'acknowledged': 0,\n",
       " 'specialists': 155,\n",
       " 'booming': 0,\n",
       " 'means': 156,\n",
       " 'uncertainty': 0,\n",
       " 'black': 157,\n",
       " 'jan.': 0,\n",
       " 'sites': 0,\n",
       " 'southmark': 0,\n",
       " 'such': 158,\n",
       " 'jobs': 159,\n",
       " 'centennial': 0,\n",
       " 'give': 160,\n",
       " 'fairly': 0,\n",
       " 'pete': 0,\n",
       " 'substantially': 0,\n",
       " 'cycles': 0,\n",
       " 'reiterated': 0,\n",
       " 'evidence': 161,\n",
       " 'four': 162,\n",
       " 'explosion': 163,\n",
       " 'partnership': 0,\n",
       " 'markets': 164,\n",
       " 'language': 0,\n",
       " 'predicts': 0,\n",
       " 'aside': 0,\n",
       " 'plant': 165,\n",
       " 'monthly': 0,\n",
       " 'stuck': 0,\n",
       " 'unit': 166,\n",
       " 'becoming': 0,\n",
       " 'proposition': 0,\n",
       " 'midst': 0,\n",
       " 'passive': 0,\n",
       " 'forcing': 0,\n",
       " 'b-<NUM>': 0,\n",
       " 'causes': 0,\n",
       " 'lack': 0,\n",
       " 'referred': 0,\n",
       " 'alex': 0,\n",
       " 'present': 0,\n",
       " 'hurricane': 0,\n",
       " 'testimony': 0,\n",
       " 'drop': 167,\n",
       " 'participate': 0,\n",
       " 'giving': 168,\n",
       " 'speak': 0,\n",
       " 'structures': 0,\n",
       " 'kohlberg': 0,\n",
       " 'j.p.': 0,\n",
       " 'kravis': 0,\n",
       " 'nuclear': 0,\n",
       " 'pentagon': 0,\n",
       " 'organization': 0,\n",
       " 'hold': 169,\n",
       " 'anticipating': 0,\n",
       " 'factor': 0,\n",
       " 'question': 170,\n",
       " 'climbed': 0,\n",
       " 'earned': 171,\n",
       " 'associations': 0,\n",
       " 'armstrong': 0,\n",
       " 'can': 172,\n",
       " 'consistently': 0,\n",
       " 'amr': 0,\n",
       " 'mortgages': 0,\n",
       " 'exempt': 0,\n",
       " 'speculate': 0,\n",
       " 'temporary': 0,\n",
       " 'ring': 0,\n",
       " 'indicating': 0,\n",
       " 'largest': 26,\n",
       " 'inc.': 173,\n",
       " 'pipeline': 0,\n",
       " 'post-crash': 0,\n",
       " 'corn': 0,\n",
       " 'similar': 174,\n",
       " 'adopt': 0,\n",
       " 'alto': 0,\n",
       " 'nwa': 0,\n",
       " 'potential': 0,\n",
       " 'cosby': 0,\n",
       " 'fourth-quarter': 0,\n",
       " 'rocked': 0,\n",
       " 'bidder': 0,\n",
       " 'expand': 0,\n",
       " 'yes': 175,\n",
       " 'licensed': 0,\n",
       " 'pay': 176,\n",
       " 'shy': 0,\n",
       " 'removal': 0,\n",
       " 'concede': 0,\n",
       " 'receive': 0,\n",
       " 'pennsylvania': 0,\n",
       " 'imposed': 0,\n",
       " 'adjusted': 0,\n",
       " 'occasion': 0,\n",
       " 'secure': 0,\n",
       " 'labor-management': 0,\n",
       " 'page': 0,\n",
       " 'thief': 0,\n",
       " 'boesel': 0,\n",
       " 'programming': 177,\n",
       " 'bet': 178,\n",
       " 'bail': 0,\n",
       " 'basis': 179,\n",
       " 'predicting': 0,\n",
       " 'surprise': 0,\n",
       " 'specifically': 0,\n",
       " 'stable': 0,\n",
       " 'trade': 180,\n",
       " 'son': 0,\n",
       " 'interviewed': 0,\n",
       " 'consumer': 181,\n",
       " 'happens': 0,\n",
       " 'pouring': 0,\n",
       " 'volatility': 0,\n",
       " 'maintaining': 0,\n",
       " 'foreign': 182,\n",
       " 'choices': 0,\n",
       " 'aviation': 0,\n",
       " 'precision': 0,\n",
       " 'real': 183,\n",
       " 'signals': 0,\n",
       " 'protest': 0,\n",
       " 'french': 184,\n",
       " 'proposals': 185,\n",
       " 'pretoria': 0,\n",
       " 'parent': 186,\n",
       " 'work': 187,\n",
       " 'wash': 0,\n",
       " 'fleet': 0,\n",
       " 'concentrate': 0,\n",
       " 'rough': 0,\n",
       " 'human': 0,\n",
       " 'manufacturers': 0,\n",
       " 'formation': 0,\n",
       " 'telesis': 0,\n",
       " 'thousand': 0,\n",
       " 'opens': 0,\n",
       " 'kenneth': 0,\n",
       " 'par': 0,\n",
       " 'pot': 0,\n",
       " 'somewhat': 188,\n",
       " 'convictions': 0,\n",
       " 'received': 189,\n",
       " 'added': 190,\n",
       " 'abruptly': 0,\n",
       " 'deficiencies': 0,\n",
       " 'developments': 0,\n",
       " 'impression': 0,\n",
       " 'parts': 0,\n",
       " 'grown': 0,\n",
       " 'allowed': 191,\n",
       " 'crown': 0,\n",
       " 'walk': 0,\n",
       " 'gop': 0,\n",
       " 'equivalent': 0,\n",
       " 'closing': 0,\n",
       " 'commissioners': 0,\n",
       " 'criticized': 0,\n",
       " 'expanded': 0,\n",
       " 'cooling': 0,\n",
       " 'finishing': 0,\n",
       " 'turn': 736,\n",
       " 'steam': 0,\n",
       " 'laptop': 0,\n",
       " 'value': 193,\n",
       " 'planted': 0,\n",
       " 'counseling': 0,\n",
       " 'premiere': 0,\n",
       " 'everybody': 0,\n",
       " 'believe': 194,\n",
       " 'eight': 195,\n",
       " 'after': 196,\n",
       " 'producers': 197,\n",
       " 'harry': 737,\n",
       " '<NUM>-month': 0,\n",
       " 'inappropriate': 0,\n",
       " 'likes': 0,\n",
       " 'deliberately': 0,\n",
       " 'eli': 0,\n",
       " 'marlin': 0,\n",
       " 'also': 199,\n",
       " 'sung': 0,\n",
       " 'furthermore': 0,\n",
       " 'compound': 0,\n",
       " 'explanation': 0,\n",
       " 'administration': 200,\n",
       " 'centerpiece': 0,\n",
       " 'editor': 0,\n",
       " 'electrical': 0,\n",
       " 'bears': 0,\n",
       " 'physicians': 0,\n",
       " 'ignore': 0,\n",
       " 'guess': 0,\n",
       " 'lawyers': 201,\n",
       " 'sheer': 0,\n",
       " 'brown': 0,\n",
       " 'peaceful': 0,\n",
       " 'like': 202,\n",
       " 'bag': 0,\n",
       " 'lenders': 0,\n",
       " 'state-owned': 0,\n",
       " 'hot': 203,\n",
       " 'advertisements': 0,\n",
       " 'pennzoil': 0,\n",
       " 'mountain': 0,\n",
       " 'topics': 0,\n",
       " 'dave': 0,\n",
       " 'loose': 0,\n",
       " 'manpower': 0,\n",
       " 'police': 0,\n",
       " 'done': 204,\n",
       " 'stop': 205,\n",
       " 'scare': 0,\n",
       " 'needs': 206,\n",
       " 'resort': 0,\n",
       " 'biotechnology': 0,\n",
       " 'passenger': 0,\n",
       " 'missile': 207,\n",
       " 'climate': 0,\n",
       " 'mistakes': 0,\n",
       " 'drew': 0,\n",
       " 'thomson': 209,\n",
       " 'experience': 0,\n",
       " 'an': 210,\n",
       " 'issue': 211,\n",
       " 'fashion': 0,\n",
       " 'formerly': 0,\n",
       " 'answer': 0,\n",
       " 'tokyo': 0,\n",
       " 'luxury-car': 0,\n",
       " 'health': 213,\n",
       " 'rebuild': 0,\n",
       " 'tv': 214,\n",
       " 'authorities': 0,\n",
       " 'ill': 0,\n",
       " 'marks': 0,\n",
       " 'downturn': 0,\n",
       " 'northwest': 0,\n",
       " 'revive': 0,\n",
       " 'patent': 0,\n",
       " 'architectural': 0,\n",
       " 'montgomery': 0,\n",
       " 'dismiss': 0,\n",
       " 'replace': 0,\n",
       " 'production': 215,\n",
       " 'chairman': 216,\n",
       " 'tends': 0,\n",
       " 'usually': 217,\n",
       " 'chose': 0,\n",
       " 'public': 1230,\n",
       " 'gardens': 0,\n",
       " 'underlying': 0,\n",
       " 'announcement': 0,\n",
       " 'conservative': 218,\n",
       " 'adjustments': 0,\n",
       " 'peak': 0,\n",
       " 'nearly': 219,\n",
       " 'persuade': 0,\n",
       " 'entry': 0,\n",
       " 'autumn': 0,\n",
       " 'visited': 0,\n",
       " 'advantage': 0,\n",
       " 'proliferation': 0,\n",
       " 'prove': 0,\n",
       " 'unchanged': 0,\n",
       " 'ferranti': 221,\n",
       " 'longtime': 0,\n",
       " 'shareholders': 222,\n",
       " 'issuing': 0,\n",
       " 'angeles-based': 0,\n",
       " 'miners': 0,\n",
       " 'men': 0,\n",
       " 'futures': 739,\n",
       " 'suggesting': 0,\n",
       " 'ancient': 0,\n",
       " 'minnesota': 0,\n",
       " 'leads': 0,\n",
       " 'accord': 0,\n",
       " 'distant': 0,\n",
       " 'quarterly': 0,\n",
       " 'estimated': 507,\n",
       " 'soviets': 0,\n",
       " 'so-called': 0,\n",
       " 'precedent': 0,\n",
       " 'alfred': 0,\n",
       " 'indonesia': 0,\n",
       " 'act': 224,\n",
       " '<unk>': 225,\n",
       " 'consulting': 0,\n",
       " 'village': 0,\n",
       " 'chinese': 0,\n",
       " 'presidential': 0,\n",
       " 'kaiser': 0,\n",
       " 'recognizing': 0,\n",
       " 'race': 0,\n",
       " 'most': 37,\n",
       " 'normally': 0,\n",
       " 'located': 0,\n",
       " 'husband': 0,\n",
       " 'correct': 0,\n",
       " 'filing': 0,\n",
       " 'seeing': 0,\n",
       " 'patterns': 0,\n",
       " 'niche': 0,\n",
       " 'two': 227,\n",
       " 'developer': 0,\n",
       " 'establishment': 0,\n",
       " 'victory': 0,\n",
       " 'personally': 0,\n",
       " 'minneapolis': 0,\n",
       " 'stock': 228,\n",
       " 'corners': 0,\n",
       " 'elderly': 0,\n",
       " 'profitably': 0,\n",
       " 'boston': 229,\n",
       " 'england': 0,\n",
       " 'connaught': 54,\n",
       " 'selling': 231,\n",
       " 'inflation-adjusted': 0,\n",
       " 'staying': 0,\n",
       " 'based': 232,\n",
       " 'dow': 233,\n",
       " 'operator': 234,\n",
       " 'title': 0,\n",
       " 'lotus': 0,\n",
       " 'seats': 0,\n",
       " 'unspecified': 0,\n",
       " 'inched': 0,\n",
       " 'really': 235,\n",
       " 'warner': 0,\n",
       " 'latter': 0,\n",
       " 'add': 0,\n",
       " 'largely': 71,\n",
       " 'matter': 237,\n",
       " 'considerations': 0,\n",
       " 'financiere': 0,\n",
       " 'ranging': 0,\n",
       " 'laurence': 0,\n",
       " 'featured': 0,\n",
       " 'club': 0,\n",
       " 'busy': 0,\n",
       " 'profit': 238,\n",
       " 'grid': 0,\n",
       " 'offset': 239,\n",
       " 'maintenance': 0,\n",
       " 'talk': 240,\n",
       " 'circuits': 0,\n",
       " 'pushing': 0,\n",
       " 'trying': 241,\n",
       " 'predictably': 0,\n",
       " 'hope': 242,\n",
       " 'junk': 243,\n",
       " 'prosecution': 0,\n",
       " 'billion': 244,\n",
       " 'others': 112,\n",
       " 'broker': 0,\n",
       " 'guy': 0,\n",
       " 'designer': 0,\n",
       " 'clark': 0,\n",
       " 'brouwer': 0,\n",
       " 'donations': 0,\n",
       " 'access': 0,\n",
       " 'industrial': 123,\n",
       " 'budgets': 0,\n",
       " 'haven': 0,\n",
       " 'comes': 246,\n",
       " 'eroding': 0,\n",
       " 'minority': 0,\n",
       " 'initiatives': 0,\n",
       " 'have': 247,\n",
       " 'gartner': 0,\n",
       " 'deployed': 0,\n",
       " 'mayor': 0,\n",
       " 'surely': 0,\n",
       " 'founded': 0,\n",
       " 'i': 248,\n",
       " 'loaded': 0,\n",
       " 'debentures': 0,\n",
       " 'setting': 0,\n",
       " 'taxpayers': 0,\n",
       " 'doctors': 0,\n",
       " 'wait': 0,\n",
       " 'compromise': 0,\n",
       " 'term': 0,\n",
       " 'kurt': 0,\n",
       " 'color': 0,\n",
       " 'industries': 250,\n",
       " 'traditional': 251,\n",
       " 'refiners': 0,\n",
       " 'practices': 0,\n",
       " 'diego': 0,\n",
       " 'sleep': 0,\n",
       " 'well-known': 0,\n",
       " 'permanently': 0,\n",
       " 'economic': 252,\n",
       " 'halt': 253,\n",
       " 'furniture': 0,\n",
       " 'rattled': 0,\n",
       " 'arthur': 0,\n",
       " 'successor': 0,\n",
       " 'policies': 0,\n",
       " 'avenue': 0,\n",
       " 'deals': 0,\n",
       " 'crop': 254,\n",
       " 'consumption': 0,\n",
       " ...}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2index[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# find words that appear in the training set so we can deal with new words separately\n",
    "count_train = np.zeros((nwords,))\n",
    "for snt in corpus_train:\n",
    "    for w in snt:\n",
    "        count_train[w2index[w]] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bi-gram Dev LL = -4.992084628169756\n"
     ]
    }
   ],
   "source": [
    "# Bigram model as a baseline\n",
    "alpha = 0.1 # add-alpha smoothing\n",
    "probB           = lm.bigramLM(corpus_train, w2index, nwords,alpha)\n",
    "LLB, N          = 0.0, 0\n",
    "bi              = lm.ngramGen(corpus_dev, w2index, 2)\n",
    "\n",
    "for w in bi:\n",
    "    if (count_train[w[1]]>0): # for now, skip target words not seen in training\n",
    "        LLB += np.log(probB[w[0], w[1]])\n",
    "        N += 1\n",
    "        \n",
    "print(\"Bi-gram Dev LL = {0}\".format(LLB / N))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Network model training:\n",
      "low freq words\n",
      "Train:\t0\tLL = -7.406177019716568\n",
      "Dev:\t0\tLL = -7.579955955251295\n"
     ]
    }
   ],
   "source": [
    "# Network model\n",
    "print(\"\\nNetwork model training:\")\n",
    "n        = 3    # Length of n-gram \n",
    "dim      = 10   # Word vector dimension\n",
    "hdim     = 30  # Hidden units\n",
    "neurallm = lm.neuralLM(dim, n, hdim, nwords)  # The network model\n",
    "\n",
    "ngrams = lm.ngramGen(corpus_train,w2index,n)\n",
    "ngrams2 = lm.ngramGen(corpus_dev,w2index,n)\n",
    "\n",
    "lrate = 0.5  # Learning rate\n",
    "for it in range(1): # passes through the training data\n",
    "    LL, N  = 0.0, 0 # Average log-likelihood, number of ngrams    \n",
    "    for ng in ngrams:\n",
    "        if ng[-1] == 0:\n",
    "            print(\"low freq words\")\n",
    "            break\n",
    "        pr = neurallm.update(ng,lrate)\n",
    "        LL += np.log(pr)\n",
    "        N  += 1\n",
    "    print('Train:\\t{0}\\tLL = {1}'.format(it, LL / N)) \n",
    "\n",
    "    #Dev set\n",
    "    LL, N = 0.0, 0 # Average log-likelihood, number of ngrams\n",
    "    for ng in ngrams2:\n",
    "        if (count_train[ng[-1]]>0): # for now, skip target words not seen in training\n",
    "            pr = neurallm.prob(ng)\n",
    "            LL += np.log(pr)\n",
    "            N  += 1\n",
    "    print('Dev:\\t{0}\\tLL = {1}'.format(it, LL / N))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1421"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(count_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "print(min(count_train))\n",
    "bi_train = lm.ngramGen(corpus_train, w2index, 2)\n",
    "ws = [a[1] for a in bi_train]\n",
    "print(np.min(count_train[ws]))\n",
    "# for w in bi_test:\n",
    "#     print(w, count_train[w[1]])\n",
    "#     if (count_train[w[1]]==0): # find words not in training set\n",
    "#         print(w)\n",
    "#     print(count_train[w[1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for w in bi:\n",
    "    \n",
    "    if (count_train[w[1]]==0): # find words not in training set\n",
    "        print(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "from __future__ import division\n",
    "\n",
    "import numpy as np\n",
    "import languagemodel as lm\n",
    "\n",
    "np.random.seed(1)  # for reproducibility\n",
    "\n",
    "corpus_train = lm.readCorpus(\"data/train.txt\")\n",
    "corpus_dev   = lm.readCorpus(\"data/dev.txt\")\n",
    "corpus_test  = lm.readCorpus(\"data/test.txt\")\n",
    "\n",
    "# build a common index (words to integers), mapping rare words (less than 5 occurences) to index 0\n",
    "# nwords = vocabulary size for the models that only see the indexes\n",
    "\n",
    "w2index,nwords = lm.buildIndex(corpus_train+corpus_dev+corpus_test)\n",
    "\n",
    "# find words that appear in the training set so we can deal with new words separately\n",
    "count_train = np.zeros((nwords,))\n",
    "for snt in corpus_train:\n",
    "    for w in snt:\n",
    "        count_train[w2index[w]] += 1\n",
    "\n",
    "# Bigram model as a baseline\n",
    "alpha = 0.1 # add-alpha smoothing\n",
    "probB           = lm.bigramLM(corpus_train, w2index, nwords,alpha)\n",
    "LLB, N          = 0.0, 0\n",
    "bi              = lm.ngramGen(corpus_dev, w2index, 2)\n",
    "for w in bi:\n",
    "    if (count_train[w[1]]>0): # for now, skip target words not seen in training\n",
    "        LLB += np.log(probB[w[0], w[1]])\n",
    "        N += 1\n",
    "print(\"Bi-gram Dev LL = {0}\".format(LLB / N))\n",
    "\n",
    "# Network model\n",
    "print(\"\\nNetwork model training:\")\n",
    "n        = 3    # Length of n-gram \n",
    "dim      = 10   # Word vector dimension\n",
    "hdim     = 30  # Hidden units\n",
    "neurallm = lm.neuralLM(dim, n, hdim, nwords)  # The network model\n",
    "\n",
    "ngrams = lm.ngramGen(corpus_train,w2index,n)\n",
    "ngrams2 = lm.ngramGen(corpus_dev,w2index,n)\n",
    "\n",
    "lrate = 0.5  # Learning rate\n",
    "for it in xrange(10): # passes through the training data\n",
    "    LL, N  = 0.0, 0 # Average log-likelihood, number of ngrams    \n",
    "    for ng in ngrams:\n",
    "        pr = neurallm.update(ng,lrate)\n",
    "        LL += np.log(pr)\n",
    "        N  += 1\n",
    "    print('Train:\\t{0}\\tLL = {1}'.format(it, LL / N)) \n",
    "\n",
    "    #Dev set\n",
    "    LL, N = 0.0, 0 # Average log-likelihood, number of ngrams\n",
    "    for ng in ngrams2:\n",
    "        if (count_train[ng[-1]]>0): # for now, skip target words not seen in training\n",
    "            pr = neurallm.prob(ng)\n",
    "            LL += np.log(pr)\n",
    "            N  += 1\n",
    "    print('Dev:\\t{0}\\tLL = {1}'.format(it, LL / N)) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "\n",
    "# ----------------------------------\n",
    "def readCorpus(filename):\n",
    "    fp = open(filename, 'r')\n",
    "    corpus = []  # list of sentences. A sentence is a lists of words.\n",
    "    for line in fp:\n",
    "        line = line.strip()\n",
    "        # replace obvious numbers with <NUM>\n",
    "        line = re.sub(r'\\b\\d+\\b', r'<NUM>', line)\n",
    "        line = re.sub(r'\\b\\d+.\\d+\\b', r'<NUM>', line)\n",
    "        corpus.append(line.split(' '))\n",
    "    return corpus\n",
    "\n",
    "# ----------------------------------\n",
    "def buildIndex(corpus, lowthreshold=5):\n",
    "\n",
    "    # initial index, to be modified later\n",
    "    tmpindex, indx = {}, 0\n",
    "    for snt in corpus:\n",
    "        for w in snt:\n",
    "            if (not tmpindex.has_key(w)):\n",
    "                tmpindex[w] = indx\n",
    "                indx += 1\n",
    "\n",
    "    # eval word counts \n",
    "    counts = np.zeros((indx,))\n",
    "    for snt in corpus:\n",
    "        for w in snt:\n",
    "            counts[tmpindex[w]] += 1\n",
    "\n",
    "    # map all the words with counts leq lowthreshold to index 0\n",
    "    newindex = {}\n",
    "    indx = 1  # 0 reserved for low occurence words\n",
    "    for w in tmpindex.keys():\n",
    "        if (counts[tmpindex[w]] <= lowthreshold):\n",
    "            newindex[w] = 0\n",
    "        else:\n",
    "            newindex[w] = indx\n",
    "            indx += 1\n",
    "            \n",
    "    # add start symbols ... <START-2> <START-1> to the index for use with up to 5-grams\n",
    "    for j in range(1, 5):\n",
    "        newindex[\"<START-\" + str(j) + \">\"] = indx\n",
    "        indx += 1\n",
    "\n",
    "    return newindex, indx\n",
    "\n",
    "# ----------------------------------\n",
    "def ngramGen(corpus, w2index, n):\n",
    "    \"\"\"ngram generator. n is the length of the ngram.\"\"\"\n",
    "    assert(n <= 5)\n",
    "    ngrams = []\n",
    "    start_snt = [\"<START-\" + str(j) + \">\" for j in range(4, 0, -1)]\n",
    "    for snt in corpus:  # sentences\n",
    "        s = start_snt[-n + 1:] + snt\n",
    "        for i in xrange(n - 1, len(s)):\n",
    "            ngrams.append([w2index[w] for w in s[i - n + 1:i + 1]])\n",
    "    return ngrams\n",
    "\n",
    "# -----------------------------------\n",
    "def unigramLM(corpus, w2index, nwords):\n",
    "    uni  = ngramGen(corpus, w2index, 1)\n",
    "    prob = np.zeros((nwords,))\n",
    "    for w in uni:\n",
    "        prob[w[0]] += 1\n",
    "    return prob / float(np.sum(prob))\n",
    "\n",
    "# -----------------------------------\n",
    "def bigramLM(corpus, w2index, nwords, alpha=0.0):\n",
    "    bi   = ngramGen(corpus, w2index, 2)\n",
    "    prob = np.zeros((nwords,nwords))+alpha\n",
    "    for w in bi:\n",
    "        prob[w[0], w[1]] += 1.0\n",
    "    for i in xrange(nwords):\n",
    "        prob[i, :] /= np.sum(prob[i, :])\n",
    "    return prob\n",
    "\n",
    "# =====================================\n",
    "class softmax(object):\n",
    "    def __init__(self, dim, nwords):\n",
    "        self.nwords = nwords    # output dim\n",
    "        self.dim    = dim       # input dim       \n",
    "        self.Wo     = np.zeros((self.nwords,))\n",
    "        self.W      = np.random.randn(self.dim, self.nwords) / np.sqrt(self.dim)\n",
    "        self.prob   = np.ones((self.nwords,)) / float(self.nwords)\n",
    "        self.G2o    = 1e-12 * np.ones((self.nwords,)) # adagrad sum squared gradients for Wo\n",
    "        self.G2     = 1e-12 * np.ones((self.nwords,)) # adagrad sum squared gradients for W\n",
    "        \n",
    "    def apply(self, x):\n",
    "        z           = self.Wo + np.dot(x, self.W)\n",
    "        self.prob   = np.exp(z - np.max(z))\n",
    "        self.prob  /= np.sum(self.prob)\n",
    "        return self.prob\n",
    "\n",
    "    # update bias, accum wordvec gradient, return dlogP[y]/dx\n",
    "    def backprop(self, x, lrate, y):\n",
    "        grad       = -self.prob\n",
    "        grad[y]   += 1.0  # dlogP[y]/dz\n",
    "        xdelta     = np.dot(self.W, grad)  # dlogP[y]/dx\n",
    "        xnorm2     = np.sum(x ** 2)\n",
    "        self.G2o  += grad ** 2\n",
    "        self.G2   += xnorm2 * grad ** 2\n",
    "        self.Wo   += lrate * grad / np.sqrt(self.G2o)\n",
    "        self.W    += lrate * np.outer(x, grad / np.sqrt(self.G2))\n",
    "        return xdelta\n",
    "\n",
    "# =====================================\n",
    "class NNlayer(object):\n",
    "    def __init__(self, idim, odim):\n",
    "        self.idim = idim\n",
    "        self.odim = odim\n",
    "        self.W    = np.random.randn(self.idim, self.odim) / np.sqrt(self.idim)\n",
    "        self.Wo   = np.zeros(self.odim,)\n",
    "        # adaGrad sum squared gradients\n",
    "        self.G2o  = 1e-12 * np.ones((self.odim,))\n",
    "        self.G2   = 1e-12 * np.ones((self.odim,))\n",
    "        self.f    = np.zeros((self.odim,))  # activation of output units\n",
    "\n",
    "    def apply(self, x):\n",
    "        self.f = np.tanh(self.Wo + np.dot(x, self.W))\n",
    "        return self.f \n",
    "\n",
    "    def backprop(self, x, lrate, delta):\n",
    "        grad       = (1.0 - self.f ** 2) * delta  # dJ/dz = df/dz * delta.  (dtanh/dx = 1 - tanh^2) \n",
    "        xdelta     = np.dot(self.W, grad)         # dJ/dx to be returned\n",
    "        xnorm2     = np.sum(x ** 2)\n",
    "        self.G2o  += grad ** 2\n",
    "        self.G2   += xnorm2 * grad ** 2\n",
    "        self.Wo   += lrate * grad / np.sqrt(self.G2o)\n",
    "        self.W    += lrate * np.outer(x, grad / np.sqrt(self.G2))\n",
    "        return xdelta\n",
    "\n",
    "# =====================================\n",
    "class neuralLM(object):\n",
    "    def __init__(self, dim, ngram, hdim, nwords):\n",
    "        self.dim    = dim       # word vector dimension\n",
    "        self.ncond  = ngram - 1 # number of conditioning words\n",
    "        self.hdim   = hdim      # number of hidden layer units\n",
    "        self.nwords = nwords    # vocab size\n",
    "\n",
    "        self.wvec    = np.random.randn(self.nwords, self.dim)  # word vectors\n",
    "        self.G2      = 1e-12 * np.ones((self.nwords,))         # adaGrad sum of squares for word vectors\n",
    "        self.hiddenL = NNlayer(self.ncond * self.dim, self.hdim)\n",
    "        self.outputL = softmax(self.hdim, self.nwords)\n",
    "        \n",
    "    def prob(self, ngram):\n",
    "        xgram, y = ngram[:-1], ngram[-1]\n",
    "        x        = np.concatenate(self.wvec[xgram, :])\n",
    "        fh       = self.hiddenL.apply(x)\n",
    "        proba    = self.outputL.apply(fh)\n",
    "        return proba[y]\n",
    "\n",
    "    def update(self, ngram, lrate):\n",
    "        # Propagate (i.e. feed-forward pass)\n",
    "        xgram, y = ngram[:-1], ngram[-1]\n",
    "        x        = np.concatenate(self.wvec[xgram, :])\n",
    "        fh       = self.hiddenL.apply(x)\n",
    "        pr       = self.outputL.apply(fh)\n",
    "        # Backpropagate (and update layers)\n",
    "        dh       = self.outputL.backprop(fh, lrate, y)\n",
    "        dx       = self.hiddenL.backprop(x, lrate, dh)\n",
    "        # Update word vectors\n",
    "        grad     = np.reshape(dx, (self.ncond, self.dim))\n",
    "        for i in xrange(self.ncond):\n",
    "            self.G2[xgram[i]]      += np.sum(grad[i, :] ** 2)\n",
    "            self.wvec[xgram[i], :] += lrate * grad[i, :] / np.sqrt(self.G2[xgram[i]])\n",
    "\n",
    "        return pr[y]"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
